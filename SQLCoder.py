"""lanchain wrapper around defog/sqlcoder"""
from typing import Any, Dict, List, Mapping, Optional, Set

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.pydantic_v1 import Field, root_validator
from langchain.llms.utils import enforce_stop_tokens


class SQLCoder(LLM):

    # model init
    model: str
    local_files_only: bool = False  # for model only
    torch_dtype: Optional[torch.dtype] = torch.bfloat16,
    load_in_8bit: Optional[bool] = False,
    load_in_4bit: Optional[bool] = False,
    device_map: Optional[str] = "auto",
    use_cache: Optional[bool] = True,

    # model
    tokenizer: Any = None
    client: Any = None

    # model params
    num_return_sequences: Optional[int] = 1
    eos_token: Optional[str] = Field(
        "```", description="End of sequence token used in prompt")
    eos_token_id: None = Field(
        None, description="Automatically calculated by tokenizer, should not supply")
    max_new_tokens: Optional[int] = 500
    do_sample: Optional[bool] = False
    num_beams: Optional[int] = 5

    @staticmethod
    def _model_param_names() -> Set[str]:
        return {
            "num_return_sequences",
            "eos_token_id",
            "pad_token_id",
            "max_new_tokens",
            "do_sample",
            "num_beams"
        }

    def _default_params(self) -> Dict[str, Any]:
        return {
            "num_return_sequences": self.num_return_sequences,
            "eos_token_id": self.eos_token_id,
            "pad_token_id": self.eos_token_id,
            "max_new_tokens": self.max_new_tokens,
            "do_sample": self.do_sample,
            "num_beams": self.num_beams
        }

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        r"""validate and initialize env for SQLCoder model"""

        if not torch.cuda.is_available():
            raise RuntimeError("Cuda not available")

        try:
            values["tokenzier"] = AutoTokenizer.from_pretrained(
                values["model"])

            values["eos_token_id"] = values["tokenizer"].convert_tokens_to_ids([values["eos_token"]])[
                0]

            if values["eos_token_id"] == 0:
                raise ValueError(
                    "End of sequence token (eos_token) not in tokenizer's vocab")

            if values["load_in_8bit"] and values["load_in_4bit"]:
                raise ValueError("Load in both 4 and 8 bit not allowed")

            values["client"] = AutoModelForCausalLM.from_pretrained(
                values["model"],
                local_files_only=values["local_files_only"],
                torch_dtype=values["torch_dtype"],
                load_in_8bit=values["load_in_8bit"],
                load_in_4bit=values["load_in_4bit"],
                device_map=values["device_map"],
                use_cache=values["use_cache"],
            )
        except Exception as e:
            raise Exception(f"Error {e}")

        return values

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {
            "model": self.model,
            **self._default_params(),
            **{
                k: v for k, v in self.__dict__.items() if k in self._model_param_names()
            },
        }

    @property
    def _llm_type(self) -> str:
        return f"SQLCoder"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        r"""Call out to SQLCoder's generate method.

        Args:
            prompt: The prompt to pass into the model. Should already be processed.
            stop: A list of strings to stop generation when encountered.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                prompt = "Once upon a time, "
                response = model(prompt, max_new_tokens=500)
        """
        if stop is not None:
            raise ValueError("stop kwargs are not required")

        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        params = {**self._default_params(), **kwargs}

        generated_ids = self.client.generate(
            **inputs,
            **params
        )
        text = self.tokenizer.batch_decode(
            generated_ids, skip_special_tokens=True)
        if stop is not None:
            text = enforce_stop_tokens(text, stop)
        return text

    # REVIEW: necessary?
    # def __del__(self):
    #     torch.cuda.empty_cache()
