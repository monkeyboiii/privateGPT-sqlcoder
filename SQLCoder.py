"""lanchain wrapper around defog/sqlcoder"""
from typing import Any, Dict, List, Mapping, Optional, Set
from dotenv import load_dotenv
import os

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.pydantic_v1 import root_validator
from langchain.llms.utils import enforce_stop_tokens


load_dotenv()


# loading
local_files_only = os.environ.get("LOCAL_FILES_ONLY", "true").lower() == "true"
torch_dtype = torch.bfloat16
load_in_8bit = os.environ.get("LOAD_IN_8BIT", "false").lower() == "true"
load_in_4bit = os.environ.get("LOAD_IN_4BIT", "false").lower() == "true"
device_map = os.environ.get("DEVICE_MAP", "auto")
use_cache = os.environ.get("LOAD_IN_8BIT", "true").lower() == "true"

# running
max_new_tokens = int(os.environ.get("MAX_NEW_TOKENS", "2048"))


class SQLCoder(LLM):

    # model init
    model: str
    # model
    tokenizer: AutoTokenizer = None
    client: AutoModelForCausalLM = None

    # model params
    num_return_sequences: Optional[int] = 1
    eos_token: str = "```"  # End of sequence token used in prompt
    eos_token_id: int = 0  # Automatically calculated by tokenizer, should not supply
    max_new_tokens: Optional[int] = 2048
    do_sample: Optional[bool] = False
    num_beams: Optional[int] = 5

    @staticmethod
    def _model_param_names() -> Set[str]:
        return {
            "num_return_sequences",
            "eos_token_id",
            "pad_token_id",
            "max_new_tokens",
            "do_sample",
            "num_beams"
        }

    def _default_params(self) -> Dict[str, Any]:
        return {
            "num_return_sequences": self.num_return_sequences,
            "eos_token_id": self.eos_token_id,
            "pad_token_id": self.eos_token_id,
            "max_new_tokens": self.max_new_tokens,
            "do_sample": self.do_sample,
            "num_beams": self.num_beams
        }

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        r"""validate and initialize env for SQLCoder model"""
        if not torch.cuda.is_available():
            raise RuntimeError("Cuda not available")

        try:
            values["tokenizer"] = AutoTokenizer.from_pretrained(
                values["model"])

            values["eos_token_id"] = values["tokenizer"].convert_tokens_to_ids([values["eos_token"]])[
                0]

            if values["eos_token_id"] == 0:
                raise ValueError(
                    "End of sequence token (eos_token) not in tokenizer's vocab")

            if values["load_in_8bit"] and values["load_in_4bit"]:
                raise ValueError("Load in both 4 and 8 bit not allowed")

            values["client"] = AutoModelForCausalLM.from_pretrained(
                values["model"],
                local_files_only=values.get(
                    "local_files_only", local_files_only),
                torch_dtype=values.get("torch_dtype", torch_dtype),
                load_in_8bit=values.get("load_in_8bit", load_in_8bit),
                load_in_4bit=values.get("load_in_4bit", load_in_4bit),
                device_map=values.get("device_map", device_map),
                use_cache=values.get("use_cache", use_cache),
            )
        except Exception as e:
            raise Exception(f"Error {e}")

        return values

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {
            "model": self.model,
            **self._default_params(),
            **{
                k: v for k, v in self.__dict__.items() if k in self._model_param_names()
            },
        }

    @property
    def _llm_type(self) -> str:
        return f"SQLCoder"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        r"""Call out to SQLCoder's generate method.

        Args:
            prompt: The prompt to pass into the model. Should already be processed.
            stop: A list of strings to stop generation when encountered.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                prompt = "Once upon a time, "
                response = model(prompt, max_new_tokens=500)
        """
        # Use custom eos token in vocab
        eos_prompt = prompt.strip(stop[0] if stop else None) + self.eos_token
        inputs = self.tokenizer(eos_prompt, return_tensors="pt").to("cuda")
        params = {**self._default_params(), **kwargs}
        generated_ids = self.client.generate(
            **inputs,
            **params
        )
        outputs = self.tokenizer.batch_decode(
            generated_ids, skip_special_tokens=True)

        if self.verbose:
            print("[*] Debug ouputs:")
            print(outputs)

        # Trim output
        # if stop is not None:
        #     result = enforce_stop_tokens(outputs[0], stop)
        sql_cmd = outputs[0].split(
            self.eos_token, 1)[1].split(self.eos_token)[0].split(";")[0].strip() + ";"

        return sql_cmd

    # REVIEW: necessary?
    # def __del__(self):
    #     torch.cuda.empty_cache()
