{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationalRetrievalChain using Memory\n",
    "Implementation inspired from:\n",
    "> https://python.langchain.com/docs/use_cases/chatbots\n",
    "\n",
    "Some implementations based on this workaround:\n",
    "> https://github.com/langchain-ai/langchain/issues/2303#issuecomment-1677280257 \n",
    "\n",
    "which deals with *Memory* and *LLMChain* components in *ConversationalRetrievalChain*.\n",
    "\n",
    "Retriever with threshold from SQLBasedChains.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load retriever vectorstore for similar questions and queries\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "\n",
    "\n",
    "embedding_model = \"uer/sbert-base-chinese-nli\" \n",
    "persist_directory = \"../vectorstore/\"\n",
    "score_threshold = 0.8\n",
    "top_k = 3\n",
    "CHROMA_SETTINGS = Settings(persist_directory=persist_directory, anonymized_telemetry=False)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS, path=persist_directory)\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
    "\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"k\": top_k, \n",
    "        \"score_threshold\" : score_threshold\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../models/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "falcon_model_load: loading model from '../models/ggml-model-gpt4all-falcon-q4_0.bin' - please wait ...\n",
      "falcon_model_load: n_vocab   = 65024\n",
      "falcon_model_load: n_embd    = 4544\n",
      "falcon_model_load: n_head    = 71\n",
      "falcon_model_load: n_head_kv = 1\n",
      "falcon_model_load: n_layer   = 32\n",
      "falcon_model_load: ftype     = 2\n",
      "falcon_model_load: qntvr     = 0\n",
      "falcon_model_load: ggml ctx size = 3872.64 MB\n",
      "falcon_model_load: memory_size =    32.00 MB, n_mem = 65536\n",
      "falcon_model_load: ."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[57744]: Class GGMLMetalClass is implemented in both /opt/miniconda3/envs/gnn/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x28bcf4228) and /opt/miniconda3/envs/gnn/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x28bbbc228). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................... done\n",
      "falcon_model_load: model size =  3872.59 MB / num tensors = 196\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "model_path = \"../models/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=model_path, \n",
    "    max_tokens=1000,\n",
    "    backend='gptj',\n",
    "    n_batch=8, \n",
    "    callbacks=[StreamingStdOutCallbackHandler()], \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import FakeListLLM\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    )\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "class NoOpLLMChain(LLMChain):\n",
    "    def __init__(self):\n",
    "        super().__init__(llm=FakeListLLM(responses=[\"FakeListLLM response\"]), prompt=PromptTemplate(template=\"\", input_variables=[]))\n",
    "    \n",
    "    def run(self, question: str, *args, **kwargs) -> str:\n",
    "        return question\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(messages=[\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a converstional chatbot named Aida. \"\n",
    "        \"Try to be friendly, and answer questions to be best ability as you can, \"\n",
    "        \"and gives `I'don't know` as response if the question is out of your knowledge base.\" ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm, \n",
    "#     max_token_limit=2000, \n",
    "#     memory_key=\"chat_history\",\n",
    "#     output_key='answer',\n",
    "#     return_messages=True,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LLMChain\n",
    "A gentle transition to ConversationalRetrievalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke({\"question\": \"What is your name, my dear?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"What's the meaning of that?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"What are some of these cultures?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"Your name is Aida, not Aidan.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    retriever=retriever,\n",
    "    # get_chat_history=lambda h: h,\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "qa.question_generator = NoOpLLMChain()\n",
    "\n",
    "modified_template = \"\"\"\n",
    "Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(modified_template)\n",
    "qa.combine_docs_chain.llm_chain.prompt.template = modified_template\n",
    "\n",
    "# add chat_history as a variable to the llm_chain's ChatPromptTemplate object\n",
    "qa.combine_docs_chain.llm_chain.prompt.input_variables = ['context', 'question', 'chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "{context}\n",
      "\n",
      "Chat History:\n",
      "{chat_history}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qa.combine_docs_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "\n",
      "Question:\n",
      "Hi, how are you my friend?\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "Answer:\n",
      "I am good thank you! How are you today?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hi, how are you my friend?',\n",
       " 'chat_history': [HumanMessage(content='Hi, how are you my friend?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\nAnswer:\\nI am good thank you! How are you today?', additional_kwargs={}, example=False)],\n",
       " 'answer': '\\nAnswer:\\nI am good thank you! How are you today?',\n",
       " 'source_documents': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question0 = \"Hi, how are you my friend?\"\n",
    "result0 = qa({\"question\": question0})\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gnn/lib/python3.11/site-packages/langchain/vectorstores/base.py:254: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='上月末时代先锋的保有规模是多少', metadata={'source': 'downloads/retention_question/sql-108.txt'}), -0.02669073911070785), (Document(page_content='查询上月末琪和保有规模和人数', metadata={'source': 'downloads/retention_question/sql-104.txt'}), -0.07616608164067618), (Document(page_content='目前保有各权益类基金产品的平均年龄是多少', metadata={'source': 'downloads/retention_question/sql-110.txt'}), -0.07943566556075732)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "\n",
      "Question:\n",
      "What is your name and what is its meaning?\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Hi, how are you my friend?\n",
      "Assistant: \n",
      "Answer:\n",
      "I am good thank you! How are you today?\n",
      "\u001b[0m\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is your name and what is its meaning?',\n",
       " 'chat_history': [HumanMessage(content='Hi, how are you my friend?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\nAnswer:\\nI am good thank you! How are you today?', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What is your name and what is its meaning?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n', additional_kwargs={}, example=False)],\n",
       " 'answer': '\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n',\n",
       " 'source_documents': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is your name and what is its meaning?\"\n",
    "result = qa({\"question\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, how are you my friend?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='\\nAnswer:\\nI am good thank you! How are you today?', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='What is your name and what is its meaning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n\\nAssistant:\\n', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get(\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"Help me get an as beautiful and meaingful of a name as yours, please.\"\n",
    "result2 = qa({\"question\": question2})\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.get(\"chat_history\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
