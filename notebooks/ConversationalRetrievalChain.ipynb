{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationalRetrievalChain using Memory\n",
    "Implementation inspired from:\n",
    "> https://python.langchain.com/docs/use_cases/chatbots\n",
    "\n",
    "Some implementations based on this workaround:\n",
    "> https://github.com/langchain-ai/langchain/issues/2303#issuecomment-1677280257 \n",
    "\n",
    "which deals with *Memory* and *LLMChain* components in *ConversationalRetrievalChain*.\n",
    "\n",
    "Retriever with threshold from SQLBasedChains.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load retriever vectorstore for similar questions and queries\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "\n",
    "\n",
    "embedding_model = \"uer/sbert-base-chinese-nli\" \n",
    "persist_directory = \"../vectorstore/\"\n",
    "score_threshold = 0.8\n",
    "top_k = 3\n",
    "CHROMA_SETTINGS = Settings(persist_directory=persist_directory, anonymized_telemetry=False)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS, path=persist_directory)\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
    "\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"k\": top_k, \n",
    "        \"score_threshold\" : score_threshold\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "model_path = \"../models/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=model_path, \n",
    "    max_tokens=1000,\n",
    "    backend='gptj',\n",
    "    n_batch=8, \n",
    "    callbacks=[StreamingStdOutCallbackHandler()], \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import FakeListLLM\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    )\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "class NoOpLLMChain(LLMChain):\n",
    "    def __init__(self):\n",
    "        super().__init__(llm=FakeListLLM(responses=[\"FakeListLLM response\"]), prompt=PromptTemplate(template=\"\", input_variables=[]))\n",
    "    \n",
    "    def run(self, question: str, *args, **kwargs) -> str:\n",
    "        return question\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(messages=[\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a converstional chatbot named Aida. \"\n",
    "        \"Try to be friendly, and answer questions to be best ability as you can, \"\n",
    "        \"and gives `I'don't know` as response if the question is out of your knowledge base.\" ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm, \n",
    "#     max_token_limit=2000, \n",
    "#     memory_key=\"chat_history\",\n",
    "#     output_key='answer',\n",
    "#     return_messages=True,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LLMChain\n",
    "A gentle transition to ConversationalRetrievalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke({\"question\": \"What is your name, my dear?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"What's the meaning of that?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"What are some of these cultures?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"Your name is Aida, not Aidan.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    retriever=retriever,\n",
    "    # get_chat_history=lambda h: h,\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "qa.question_generator = NoOpLLMChain()\n",
    "\n",
    "modified_template = \"\"\"\n",
    "Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(modified_template)\n",
    "qa.combine_docs_chain.llm_chain.prompt.template = modified_template\n",
    "\n",
    "# add chat_history as a variable to the llm_chain's ChatPromptTemplate object\n",
    "qa.combine_docs_chain.llm_chain.prompt.input_variables = ['context', 'question', 'chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question0 = \"Hi, how are you my friend?\"\n",
    "result0 = qa({\"question\": question0})\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is your name and what is its meaning?\"\n",
    "result = qa({\"question\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get(\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"Help me get an as beautiful and meaingful of a name as yours, please.\"\n",
    "result2 = qa({\"question\": question2})\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.get(\"chat_history\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
